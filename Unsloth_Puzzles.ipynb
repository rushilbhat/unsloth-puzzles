{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushilbhat/unsloth-puzzles/blob/main/Unsloth_Puzzles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uwPWn_fCGFo"
      },
      "source": [
        "### ðŸ¦¥ Unsloth is growing! Come join us :)\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a>\n",
        "\n",
        "Up to $500K USD salary + bonus equity, health care benefits + other benefits, USA relocation etc! Complete some puzzles and earn points!\n",
        "\n",
        "* We encourage you to use AI for coding!<ins> No experience or PhD / Masters needed</ins> - just get enough points for consideration!\n",
        "* There are <ins>negative points</ins> for incorrect submissions. Read each criteria! Read [Submission](#SUBMISSION) steps.\n",
        "\n",
        "| Role              | Compensation   | Role Description | Points Needed |\n",
        "| ----------------- | -------------- | ----------- | --- |\n",
        "| Founding Engineer | \\$400K to \\$500K & equity | Help push Unsloth forward - bug fixes, core features, UI, kernels, nearly anything! | 47 |\n",
        "| ML Engineer | \\$250K to \\$300K & equity | Help with FSDP2, Float8, Float4, kernels, Unsloth core and more! | 32 |\n",
        "| ML Intern | up to \\$150K py | Implementing specific features in Unsloth core. Can be remote.  | 18 |\n",
        "\n",
        "1. [Convert `nf4` to Triton](#NF4) [Difficulty: Hard] [Max points: 14]\n",
        "2. [Make `QLoRA` work with `FSDP2`](#FSDP2) [Difficulty: Medium to Hard] [Max points: 12]\n",
        "3. [Make `torch.compile` work without graph breaks for QLoRA](#COMPILE) [Difficulty: Easy to Medium] [Max points: 9]\n",
        "4. [Help solve ðŸ¦¥ Unsloth issues!](#ISSUES) [Difficulty: Varies] [Max points: 12]\n",
        "5. [Memory Efficient Backprop](#MATH) [Difficulty: Medium to Hard] [Max points: 10]\n",
        "6. [Submission steps](#SUBMISSION)\n",
        "\n",
        "### ðŸ¦¥ Who are we?\n",
        "* 1.58bit DeepSeek R1 GGUFs [Tweet](https://x.com/UnslothAI/status/1883899061893546254) and [HF Model Page](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)\n",
        "* GRPO Llama 3.1 8B on a free Colab [Tweet](https://x.com/UnslothAI/status/1887562753126408210)\n",
        "* Gemma bug fixes [Tweet](https://x.com/danielhanchen/status/1765446273661075609) and bug fixes for Llama 3, Phi 3, Qwen 2.5 [Details](https://unsloth.ai/blog/phi3) Llama-fying Phi-4 [Details](https://unsloth.ai/blog/phi4)\n",
        "* Gradient accumulation bug fixes [Tweet](https://x.com/danielhanchen/status/1846235913443262891) 4bit Dynamic Quantization [Details](https://unsloth.ai/blog/dynamic-4bit)\n",
        "* Unsloth Gradient Checkpointing async offloads activations [Details](https://unsloth.ai/blog/long-context)\n",
        "* 30K Github Stars [Github](https://github.com/unslothai/unsloth) & 7 million monthly downloads on [Hugging Face](https://huggingface.co/unsloth)\n",
        "* PyTorch conference [video](https://www.youtube.com/watch?v=PdtKkc5jB4g) AI Engineer World's Fair [video](https://www.youtube.com/watch?v=pRM_P6UfdIc) GPU / CUDA MODE [talk](https://www.youtube.com/watch?v=hfb_AIhDYnA)\n",
        "\n",
        "\n",
        "### Clarifications:\n",
        "1. We'll compensate you if we interview you but don't hire you\n",
        "2. \\$100-\\$1000 bounties for Task 4\n",
        "3. Submissions must be Apache-2 licensed\n",
        "4. Task 4 involves solving Github issues for OSS Unsloth\n",
        "5. No time limit: rolling basis\n",
        "6. US based preferred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EI_d4FLkR51i"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=0.01, rtol=0.01)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoE2DGRZG2Ng"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"NF4\"></a>\n",
        "## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n",
        "\n",
        "1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n",
        "2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n",
        "3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n",
        "4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n",
        "5. Use `test_dequantize_function` to test your implementation.\n",
        "6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n",
        "7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WKQ9hdqNOXpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b403a1-ae6a-4b1f-f516-138c78fe5bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EiO1cu2YKB"
      },
      "source": [
        "For example, we can test our implementation via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM8q3rDX1XfZ",
        "outputId": "217df52f-9b82-461f-97e8-9addb76ce676"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.583423376083374"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETwlex22lMN"
      },
      "source": [
        "The elapsed time for our implementation over 1000 trials is 5.38 seconds or so.\n",
        "\n",
        "PEFT also has one, which should be mostly identical to Unsloth's version, albeit slightly slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5RShLO1h-Y",
        "outputId": "50ae9171-1fa5-4f31-e120-d7cc0eff07c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.013213634490967"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5pUaSN3JcM"
      },
      "source": [
        "Write your Triton kernel below, and test it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(\n",
        "    weight_ptr, # *uint8\n",
        "    code_ptr, # *float32\n",
        "    code2_ptr, # *float32\n",
        "    absmax_ptr, # *uint8\n",
        "    absmax2_ptr, # *float32\n",
        "    offset_ptr, # *float32\n",
        "    out_ptr, # *fp16\n",
        "    N_out,\n",
        "    weight_block_size,\n",
        "    absmax_block_size,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < N_out\n",
        "\n",
        "    block_id = offsets // weight_block_size\n",
        "    block2_id = block_id // absmax_block_size\n",
        "\n",
        "    # Dequantise absmax\n",
        "    quant_abs = tl.load(absmax_ptr + block_id, mask=mask, other=0.0).to(tl.int32)\n",
        "    norm_abs = tl.load(code2_ptr + quant_abs, mask=mask, other=0.0)\n",
        "    abs2 = tl.load(absmax2_ptr + block2_id, mask=mask, other=0.0)\n",
        "    abs_offset = tl.load(offset_ptr)\n",
        "    abs = (norm_abs * abs2) + abs_offset\n",
        "\n",
        "    # Dequantise weight\n",
        "    is_high = (offsets % 2) == 0\n",
        "    shift = tl.where(is_high, 4, 0)\n",
        "\n",
        "    byte = tl.load(weight_ptr + offsets//2, mask=mask, other=0.0)\n",
        "    nibble = (byte >> shift) & 0xF\n",
        "\n",
        "    norm_w = tl.load(code_ptr + nibble, mask=mask, other=0.0)\n",
        "    dequant_w = norm_w * abs\n",
        "\n",
        "    out_val = tl.cast(dequant_w, tl.float16)\n",
        "\n",
        "    tl.store(out_ptr + offsets, out_val, mask=mask)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    shape = quant_state.shape\n",
        "    dtype = torch.float16\n",
        "    out = torch.empty(shape, dtype=dtype, device=weight.device)\n",
        "\n",
        "    N_out = out.numel()\n",
        "\n",
        "    BLOCK = 1024\n",
        "    grid = lambda meta: (triton.cdiv(N_out, BLOCK), )\n",
        "\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight,\n",
        "        quant_state.code,\n",
        "        quant_state.state2.code,\n",
        "        quant_state.absmax,\n",
        "        quant_state.state2.absmax,\n",
        "        quant_state.offset,\n",
        "        out,\n",
        "        N_out,\n",
        "        quant_state.blocksize,\n",
        "        quant_state.state2.blocksize,\n",
        "        BLOCK,\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
      ],
      "metadata": {
        "id": "9YNTqTGBgn5I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NvvEm6ZH35fB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19292ca-6901-44e6-e9dc-00e0ba2c5c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 3.345266819000244\n",
            "Speedup: 1.4250393215288855\n"
          ]
        }
      ],
      "source": [
        "### TEST IT BELOW:\n",
        "print(f\"Elapsed time: {test_dequantize(your_dequantize_nf4)}\")\n",
        "\n",
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "print(f\"Speedup: {test_dequantize(unsloth_dequantize) / test_dequantize(your_dequantize_nf4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYaff1Ror8R_"
      },
      "source": [
        "## Marking Criteria for A) Max points = 14\n",
        "```python\n",
        "if attemped_A:\n",
        "    A_score = 0\n",
        "    if single_triton_kernel: A_score += 3\n",
        "    speedup = old_time / new_time\n",
        "    if speedup <= 1.00: A_score -= 3\n",
        "    if speedup >= 1.05: A_score += 1\n",
        "    if speedup >= 1.10: A_score += 2\n",
        "    if speedup >= 1.15: A_score += 2\n",
        "    if kernel_works_in_torch_compile: A_score += 1\n",
        "    else: A_score -= 1\n",
        "    if custom_asm_works: A_score += 3\n",
        "    if uses_cache_eviction: A_score += 1\n",
        "    if tested_in_f16_and_bf16: A_score += 1\n",
        "    else: A_score -= 1\n",
        "    final_score += A_score\n",
        "else:\n",
        "    final_score += 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXshnajO44Kb"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"FSDP2\"></a>\n",
        "## B) Make `QLoRA` work with `FSDP2` - N/A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vIg_ZjYt1Tq"
      },
      "source": [
        "## Marking Criteria for B) Max points = 10\n",
        "```python\n",
        "if attemped_B:\n",
        "    B_score = 0\n",
        "    if FSDP2_works_with_QLoRA:\n",
        "        if torch_compile_works: B_score += 5\n",
        "        else: B_score += 3\n",
        "        if uses_part_A_and_single_kernel_and_faster: B_score += 3\n",
        "        elif uses_torchAO:\n",
        "            if torchAO_slower_than_BnB: B_score -= 3\n",
        "    elif TP_or_PP_with_QLoRA:\n",
        "        if zero_bubble: B_score += 3\n",
        "        else: B_score += 2\n",
        "    elif FSDP1_works_with_QLoRA:\n",
        "        B_score += 1\n",
        "    if kaggle_notebook_2_tesla_t4_example:\n",
        "        B_score += 2\n",
        "    else:\n",
        "        B_score = 0\n",
        "    final_score += B_score\n",
        "else:\n",
        "    final_score -= 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pukEsR2YnIHQ"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"COMPILE\"></a>\n",
        "## C) Make `torch.compile` work without graph breaks for QLoRA [Difficulty: Easy to Medium] [Max points: 9]\n",
        "\n",
        "1. Goal: Write a single Python script like task B), except the goal is to `torch.compile` all modules if possible.\n",
        "\n",
        "2. There must NOT be graph breaks, and excessive re-compilations should not be seen.\n",
        "\n",
        "3. You should have say max 30 compilations. Over 60 is definitely wrong.\n",
        "\n",
        "4. The loss must match with the non compiled module.\n",
        "\n",
        "5. Utilize patching as much as possible.\n",
        "\n",
        "6. Think about which areas might need disabling for compilation. Think about regional compilation. How do we compile sections efficiently?\n",
        "\n",
        "7. Log memory / VRAM usage, and monitor speedups as well.\n",
        "\n",
        "8. Must work for QLoRA.\n",
        "\n",
        "We provided a script below, and showcased how to detect if graph breaks are seen. We also torch compiled the MLP for Llama:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Note: `max_autotune` requires at least 80 SMs but T4 GPUs only have 40 SMs** - https://discuss.pytorch.org/t/torch-compile-warning-not-enough-sms-to-use-max-autotune-gemm-mode/184405\n",
        "\n"
      ],
      "metadata": {
        "id": "sQ8sMWNQA6_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QFOXncAVNqmK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers.models.llama.modeling_llama as llama_model\n",
        "\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "llama_model.LlamaMLP.forward = torch.compile(\n",
        "    llama_model.LlamaMLP.forward,\n",
        "    fullgraph=False,\n",
        "    dynamic=True,\n",
        "    options=torch_compile_options\n",
        ")\n",
        "\n",
        "llama_model.LlamaAttention.forward = torch.compile(\n",
        "    llama_model.LlamaAttention.forward,\n",
        "    fullgraph=False,\n",
        "    dynamic=True,\n",
        "    options=torch_compile_options\n",
        ")\n",
        "\n",
        "llama_model.LlamaRMSNorm.forward = torch.compile(\n",
        "    llama_model.LlamaRMSNorm.forward,\n",
        "    fullgraph=False,\n",
        "    dynamic=True,\n",
        "    options=torch_compile_options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WmoQzMDzm1zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399f0493-441f-4292-972a-409ceba56d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.loss_function = torch.compile(model.model.loss_function, fullgraph=False, dynamic=True, options=torch_compile_options)"
      ],
      "metadata": {
        "id": "eZR4UD56_god"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "class Linear4bitPatch(nn.Module):\n",
        "    def __init__(self, bnb_linear):\n",
        "        super().__init__()\n",
        "\n",
        "        w_4bit = bnb_linear.weight.data\n",
        "        qs = bnb_linear.weight.quant_state\n",
        "\n",
        "        if bnb_linear.bias is not None:\n",
        "            bias_data = bnb_linear.bias.detach()\n",
        "        else:\n",
        "            bias_data = None\n",
        "\n",
        "\n",
        "        self.register_buffer(\"weight_4bit\", w_4bit, persistent=False)\n",
        "        if bias_data is not None:\n",
        "            self.bias = nn.Parameter(bias_data)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.quant_state = qs\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        W_deq = _your_dequantize_nf4(self.weight_4bit, self.quant_state)\n",
        "        out = torch.nn.functional.linear(x, W_deq, self.bias)\n",
        "        return out\n",
        "\n",
        "\n",
        "def replace_4bit_linears(module: nn.Module):\n",
        "    for name, child in list(module.named_children()):\n",
        "        if isinstance(child, bnb.nn.Linear4bit):\n",
        "            new_lin = Linear4bitPatch(child)\n",
        "            setattr(module, name, new_lin)\n",
        "        else:\n",
        "            replace_4bit_linears(child)\n",
        "\n",
        "replace_4bit_linears(model)\n"
      ],
      "metadata": {
        "id": "73LhKV8TVg3E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
        "\n",
        "original_sdpa = ALL_ATTENTION_FUNCTIONS[\"sdpa\"]\n",
        "def patched_sdpa_attention_forward(*args, **kwargs):\n",
        "    kwargs[\"is_causal\"] = True\n",
        "    return original_sdpa(*args, **kwargs)\n",
        "ALL_ATTENTION_FUNCTIONS[\"sdpa\"] = patched_sdpa_attention_forward\n"
      ],
      "metadata": {
        "id": "bin65CBfwtYq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbVNGNQ5LlpJ"
      },
      "source": [
        "We provide full logging for `torch.compile` like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EsekFGdsK5hZ"
      },
      "outputs": [],
      "source": [
        "# Must show all graph breaks are not seen with torch.compile\n",
        "import os\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "import logging\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        "    # aot_joint_graph = True, # Enable for more logs\n",
        "    # aot_graphs = True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False\n",
        "\n",
        "torch._dynamo.config.cache_size_limit = 256\n",
        "# print(torch._dynamo.config.cache_size_limit)\n",
        "# print(torch._dynamo.config.accumulated_cache_size_limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RdragY7P-dL"
      },
      "source": [
        "When we execute the code below, we can see graph breaks - remove them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 10,\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        seed = 3407,\n",
        "        max_seq_length = max_seq_length,\n",
        "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
        "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
        "        report_to = \"none\", # For W&B\n",
        "        dataset_num_proc = 4,\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qx7E28TnrlGE",
        "outputId": "7c6dce5f-9fed-44bd-9bfa-eb8a8fcd5458"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0301 14:19:43.118000 9431 torch/_inductor/debug.py:434] [0/0] model__0_forward_1 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__0_forward_1.0\n",
            "W0301 14:19:43.427000 9431 torch/_inductor/debug.py:434] [0/0] model__0_backward_2 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__0_backward_2.1\n",
            "W0301 14:19:56.689000 9431 torch/_inductor/utils.py:1048] [1/0] Not enough SMs to use max_autotune_gemm mode\n",
            "W0301 14:20:00.543000 9431 torch/_inductor/debug.py:434] [1/0] model__1_forward_4 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__1_forward_4.2\n",
            "W0301 14:20:03.142000 9431 torch/_inductor/debug.py:434] [1/0] model__1_backward_5 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__1_backward_5.3\n",
            "W0301 14:20:08.402000 9431 torch/_inductor/debug.py:434] [2/0] model__2_forward_7 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__2_forward_7.4\n",
            "W0301 14:20:08.537000 9431 torch/_inductor/debug.py:434] [2/0] model__2_backward_8 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__2_backward_8.5\n",
            "V0301 14:20:08.764000 9431 torch/_dynamo/guards.py:2811] [1/1] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:20:08.764000 9431 torch/_dynamo/guards.py:2811] [1/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:20:08.764000 9431 torch/_dynamo/guards.py:2811] [1/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:20:08.764000 9431 torch/_dynamo/guards.py:2811] [1/1] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:20:21.981000 9431 torch/_inductor/debug.py:434] [1/1] model__3_forward_10 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__3_forward_10.6\n",
            "W0301 14:20:24.704000 9431 torch/_inductor/debug.py:434] [1/1] model__3_backward_11 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__3_backward_11.7\n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose] \n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:20:28.031000 9431 torch/_dynamo/guards.py:2811] [1/2] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:20:39.945000 9431 torch/_inductor/debug.py:434] [1/2] model__4_forward_13 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__4_forward_13.8\n",
            "W0301 14:20:42.494000 9431 torch/_inductor/debug.py:434] [1/2] model__4_backward_14 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__4_backward_14.9\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose] \n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose] \n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:20:46.360000 9431 torch/_dynamo/guards.py:2811] [1/3] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:21:01.263000 9431 torch/_inductor/debug.py:434] [1/3] model__5_forward_16 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__5_forward_16.10\n",
            "W0301 14:21:03.786000 9431 torch/_inductor/debug.py:434] [1/3] model__5_backward_17 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__5_backward_17.11\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose] \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose] \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose] \n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:21:06.862000 9431 torch/_dynamo/guards.py:2811] [1/4] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:21:18.687000 9431 torch/_inductor/debug.py:434] [1/4] model__6_forward_19 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__6_forward_19.12\n",
            "W0301 14:21:21.326000 9431 torch/_inductor/debug.py:434] [1/4] model__6_backward_20 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__6_backward_20.13\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose] \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose] \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose] \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose] \n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:21:25.079000 9431 torch/_dynamo/guards.py:2811] [1/5] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:21:37.500000 9431 torch/_inductor/debug.py:434] [1/5] model__7_forward_22 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__7_forward_22.14\n",
            "W0301 14:21:40.072000 9431 torch/_inductor/debug.py:434] [1/5] model__7_backward_23 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__7_backward_23.15\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose] \n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:21:43.191000 9431 torch/_dynamo/guards.py:2811] [1/6] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:21:55.549000 9431 torch/_inductor/debug.py:434] [1/6] model__8_forward_25 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__8_forward_25.16\n",
            "W0301 14:21:58.308000 9431 torch/_inductor/debug.py:434] [1/6] model__8_backward_26 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__8_backward_26.17\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose] \n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:22:01.951000 9431 torch/_dynamo/guards.py:2811] [1/7] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:22:13.736000 9431 torch/_inductor/debug.py:434] [1/7] model__9_forward_28 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__9_forward_28.18\n",
            "W0301 14:22:16.284000 9431 torch/_inductor/debug.py:434] [1/7] model__9_backward_29 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__9_backward_29.19\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose] \n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:22:19.354000 9431 torch/_dynamo/guards.py:2811] [1/8] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:22:31.821000 9431 torch/_inductor/debug.py:434] [1/8] model__10_forward_31 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__10_forward_31.20\n",
            "W0301 14:22:34.358000 9431 torch/_inductor/debug.py:434] [1/8] model__10_backward_32 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__10_backward_32.21\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose] \n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:22:38.195000 9431 torch/_dynamo/guards.py:2811] [1/9] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:22:50.089000 9431 torch/_inductor/debug.py:434] [1/9] model__11_forward_34 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__11_forward_34.22\n",
            "W0301 14:22:52.628000 9431 torch/_inductor/debug.py:434] [1/9] model__11_backward_35 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__11_backward_35.23\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose] \n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:22:55.742000 9431 torch/_dynamo/guards.py:2811] [1/10] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:23:08.286000 9431 torch/_inductor/debug.py:434] [1/10] model__12_forward_37 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__12_forward_37.24\n",
            "W0301 14:23:10.853000 9431 torch/_inductor/debug.py:434] [1/10] model__12_backward_38 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__12_backward_38.25\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/10: len(L['past_key_value'].key_cache) == 10                    \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose] \n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     guard 10 failures:\n",
            "V0301 14:23:14.697000 9431 torch/_dynamo/guards.py:2811] [1/11] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:23:26.720000 9431 torch/_inductor/debug.py:434] [1/11] model__13_forward_40 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__13_forward_40.26\n",
            "W0301 14:23:29.254000 9431 torch/_inductor/debug.py:434] [1/11] model__13_backward_41 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__13_backward_41.27\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/11: len(L['past_key_value'].key_cache) == 11                    \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/10: len(L['past_key_value'].key_cache) == 10                    \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 10 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose] \n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     guard 11 failures:\n",
            "V0301 14:23:32.326000 9431 torch/_dynamo/guards.py:2811] [1/12] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:23:44.887000 9431 torch/_inductor/debug.py:434] [1/12] model__14_forward_43 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__14_forward_43.28\n",
            "W0301 14:23:47.426000 9431 torch/_inductor/debug.py:434] [1/12] model__14_backward_44 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__14_backward_44.29\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/12: len(L['past_key_value'].key_cache) == 12                    \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/11: len(L['past_key_value'].key_cache) == 11                    \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/10: len(L['past_key_value'].key_cache) == 10                    \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 10 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 11 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose] \n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     guard 12 failures:\n",
            "V0301 14:23:51.234000 9431 torch/_dynamo/guards.py:2811] [1/13] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:24:03.141000 9431 torch/_inductor/debug.py:434] [1/13] model__15_forward_46 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__15_forward_46.30\n",
            "W0301 14:24:05.658000 9431 torch/_inductor/debug.py:434] [1/13] model__15_backward_47 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__15_backward_47.31\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/13: len(L['past_key_value'].key_cache) == 13                    \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/12: len(L['past_key_value'].key_cache) == 12                    \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/11: len(L['past_key_value'].key_cache) == 11                    \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/10: len(L['past_key_value'].key_cache) == 10                    \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 10 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 11 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 12 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose] \n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     guard 13 failures:\n",
            "V0301 14:24:08.678000 9431 torch/_dynamo/guards.py:2811] [1/14] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:24:21.309000 9431 torch/_inductor/debug.py:434] [1/14] model__16_forward_49 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__16_forward_49.32\n",
            "W0301 14:24:23.853000 9431 torch/_inductor/debug.py:434] [1/14] model__16_backward_50 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__16_backward_50.33\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:257\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 0 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/14: len(L['past_key_value'].key_cache) == 14                    \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 1 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/13: len(L['past_key_value'].key_cache) == 13                    \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 2 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/12: len(L['past_key_value'].key_cache) == 12                    \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 3 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/11: len(L['past_key_value'].key_cache) == 11                    \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 4 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/10: len(L['past_key_value'].key_cache) == 10                    \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 5 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/9: len(L['past_key_value'].key_cache) == 9                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 6 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/8: len(L['past_key_value'].key_cache) == 8                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 7 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/7: len(L['past_key_value'].key_cache) == 7                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 8 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/6: len(L['past_key_value'].key_cache) == 6                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 9 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/5: len(L['past_key_value'].key_cache) == 5                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 10 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/4: len(L['past_key_value'].key_cache) == 4                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 11 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/3: len(L['past_key_value'].key_cache) == 3                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 12 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/2: len(L['past_key_value'].key_cache) == 2                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 13 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/1: len(L['past_key_value'].key_cache) == 1                     \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose] \n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     guard 14 failures:\n",
            "V0301 14:24:27.721000 9431 torch/_dynamo/guards.py:2811] [1/15] [__recompiles_verbose]     - 1/0: not L['past_key_value'].key_cache                           \n",
            "W0301 14:24:39.528000 9431 torch/_inductor/debug.py:434] [1/15] model__17_forward_52 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__17_forward_52.34\n",
            "W0301 14:24:42.116000 9431 torch/_inductor/debug.py:434] [1/15] model__17_backward_53 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__17_backward_53.35\n",
            "W0301 14:24:46.002000 9431 torch/_inductor/debug.py:434] [3/0] model__18_forward_55 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__18_forward_55.36\n",
            "W0301 14:24:46.082000 9431 torch/_inductor/debug.py:434] [3/0] model__18_backward_56 debug trace: /content/torch_compile_debug/run_2025_03_01_14_19_39_843051-pid_9431/torchinductor/model__18_backward_56.37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.527600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.134300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.972300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.241000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.215800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.671200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.379335355758667, metrics={'train_runtime': 312.2908, 'train_samples_per_second': 0.064, 'train_steps_per_second': 0.032, 'total_flos': 240546115584.0, 'train_loss': 2.379335355758667})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbbwd5uDOL1"
      },
      "source": [
        "Log all your steps for debugging in a Colab (maybe this one). Edward's blog http://blog.ezyang.com/, Horace's blogs https://www.thonking.ai/, Slaying OOMs by Jane & Mark: ttps://www.youtube.com/watch?v=UvRl4ansfCg could be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging steps\n",
        "\n",
        "## MLP Compilation\n",
        "Started by checking the error logs and intially found two types of graph breaks:\n",
        "\n",
        "1. First error: `torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {`\n",
        "\n",
        "2. Second error: `torch._dynamo.exc.Unsupported: Tensor.data_ptr`\n",
        "\n",
        "The first error, according to the log\n",
        "\n",
        "```\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks] Graph break: from user code at:\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"<ipython-input-3-7786d8f77241>\", line 12, in compiled_llama_mlp\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
        "V0228 22:08:55.241000 2436 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
        "```\n",
        "\n",
        "happens because torch.compile canâ€™t trace transpose operation on Params4bit object in the forward pass of Linear4bit:\n",
        "```\n",
        "return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
        "```\n",
        "\n",
        "Digging further, bnb.matmul_4bit calls:\n",
        "\n",
        "```\n",
        "return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
        "```\n",
        "\n",
        "Then MatMul4Bit.forward calls:\n",
        "```\n",
        "output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
        "```\n",
        "\n",
        "Luckily, it turns out that the second graph break comes from F.dequantize_4bit, which eventually calls A.data_ptr().\n",
        "\n",
        "So to fix this, I patched Linear4bit. Before torch.compile, I extracted the raw quantized weight tensor and quant state in the constructor. Then in the forward pass, I dequantized the weight tensor using my kernel from Task A. This completely resolved all graph breaks.\n",
        "\n",
        "## Attention Compilation\n",
        "\n",
        "Started by compiling LlamaAttention.forward to find graph breaks:\n",
        "```\n",
        "import transformers.models.llama.modeling_llama as llama_model\n",
        "llama_model.LlamaAttention.forward = torch.compile(\n",
        "   llama_model.LlamaAttention.forward,\n",
        "   fullgraph=False,\n",
        "   dynamic=True,\n",
        "   options=torch_compile_options\n",
        ")\n",
        "```\n",
        "\n",
        "It was taking too long, so I temporarily kept only the first transformer block:\n",
        "```\n",
        "model.base_model.model.model.layers = model.base_model.model.model.layers[:1]\n",
        "```\n",
        "Found one graph break:\n",
        "```\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks] torch._dynamo.exc.Unsupported: TypeError <built-in function scaled_dot_product_attention>: scaled_dot_product_attention(): argument 'is_causal' must be bool, not SymBool\n",
        "```\n",
        "\n",
        "Looking at the log\n",
        "```\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks] Graph break: from user code at:\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 291, in forward\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     attn_output, attn_weights = attention_interface(\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\", line 53, in sdpa_attention_forward\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks] Traceback (most recent call last):\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2132, in run_node\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]     return node.target(*args, **kwargs)\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "V0228 23:17:29.439000 6790 torch/_dynamo/symbolic_convert.py:617] [0/0] [__graph_breaks] TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not SymBool\n",
        "```\n",
        "\n",
        "the issue comes from `query.shape[2] > 1` in sdpa_attention_forward. Torch.compile canâ€™t trace it since `query.shape[2]` can only be resolved at runtime.\n",
        "\n",
        "In our case, since the sequence length is always greater than 1, I wrapped sdpa_attention_forward in a function that sets:\n",
        "```\n",
        "kwargs[\"is_causal\"] = True\n",
        "```\n",
        "before passing it to sdpa_attention_forward. This fixed the issue.\n",
        "\n"
      ],
      "metadata": {
        "id": "i_ppjqHU7KyV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wojg8SDjv3fu"
      },
      "source": [
        "## Marking Criteria for C) Max points = 9\n",
        "```python\n",
        "if attemped_C:\n",
        "    C_score = 0\n",
        "    if uses_flex_attention:\n",
        "        if dynamic_sequence_length_works: C_score += 3\n",
        "        else: C_score += 1\n",
        "    if no_torch_compile_BnB: C_score -= 2\n",
        "    elif use_part_A: C_score += 1\n",
        "    elif torch_compile_BnB: C_score += 1\n",
        "\n",
        "    if attention_compiled:\n",
        "        if excessive_recompilation: C_score -= 3\n",
        "        else: C_score += 2\n",
        "    if mlp_compiled:\n",
        "        if excessive_recompilation: C_score -= 3\n",
        "        C_score += 1\n",
        "\n",
        "    if not loss_compiled: C_score -= 1\n",
        "    if not layernorms_compiled: C_score -= 3\n",
        "\n",
        "    if max_autotune_triton_matmul:\n",
        "        if excessive_recompilation: C_score -= 2\n",
        "        else: C_score += 2\n",
        "    \n",
        "    final_score += C_score\n",
        "else:\n",
        "    final_score -= 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKcvFLCsQLtL"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"ISSUES\"></a>\n",
        "## D) Help solve ðŸ¦¥ Unsloth issues! - N/A\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VfYZjuMxujG"
      },
      "source": [
        "## Marking Criteria for D) Max points = 12\n",
        "```python\n",
        "if attemped_D:\n",
        "    D_score = 0\n",
        "    for subtask in subtasks:\n",
        "        if sucessfully_completed_subtask:\n",
        "            D_score += score_for_subtask\n",
        "    final_score += D_score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrJzggfH2YEG"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"MATH\"></a>\n",
        "## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n",
        "\n",
        "In LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n",
        "\n",
        "For example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n",
        "\n",
        "In Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n",
        "\n",
        "Our goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\n",
        "f \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "So, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n",
        "\n",
        "Notice during backpropagation we can use the chain rule:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n",
        "\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n",
        "\\frac{dy}{dX} &= W^T \\\\\n",
        "\\frac{dy}{dW} &= X^T \\\\\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n",
        "\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n",
        "\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n",
        "\n",
        "2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n",
        "\n",
        "3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n",
        "\n",
        "4. Make the Cross Entropy Loss work. You must show other functions working as well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.loss.loss_utils import ForCausalLMLoss\n",
        "from functools import partial\n",
        "\n",
        "def transformation_function(batch, linear, labels, loss_fn):\n",
        "   batch = batch.to(linear.weight.dtype)\n",
        "   x = linear(batch).float()\n",
        "   if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "       return loss_fn(x.view(-1, x.shape[-1]), labels.view(-1))\n",
        "   elif isinstance(loss_fn, nn.MSELoss):\n",
        "       return loss_fn(x, labels)\n",
        "   else:\n",
        "       return loss_fn(x, labels)\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "   @staticmethod\n",
        "   def forward(ctx, X, linear, labels, forward_function, loss_fn, split_size, scaler=None):\n",
        "       outputs = []\n",
        "       X_grads = []\n",
        "       weight_grads = []\n",
        "\n",
        "       with torch.enable_grad():\n",
        "           X_chunks = torch.split(X, split_size, dim=0)\n",
        "           labels_chunks = torch.split(labels, split_size, dim=0)\n",
        "           num_chunks = len(X_chunks)\n",
        "\n",
        "           for chunk_idx, (X_chunk, labels_chunk) in enumerate(zip(X_chunks, labels_chunks)):\n",
        "               chunk_loss = forward_function(X_chunk, linear, labels_chunk, loss_fn)\n",
        "\n",
        "               loss_from_HF = isinstance(loss_fn, partial) and isinstance(loss_fn.func, type(ForCausalLMLoss))\n",
        "               chunk_loss /= num_chunks if not loss_from_HF else 1.0\n",
        "               outputs.append(chunk_loss)\n",
        "\n",
        "               grad_targets = (X_chunk,)\n",
        "               if linear.weight.requires_grad:\n",
        "                  grad_targets += (linear.weight,)\n",
        "               if scaler is not None:\n",
        "                  chunk_loss = scaler.scale(chunk_loss)\n",
        "\n",
        "               grads = torch.autograd.grad(chunk_loss,\n",
        "                                           grad_targets,\n",
        "                                           allow_unused=True\n",
        "               )\n",
        "               X_grads.append(grads[0])\n",
        "               if linear.weight.requires_grad:\n",
        "                  weight_grads.append(grads[1])\n",
        "\n",
        "           output = sum(outputs)\n",
        "\n",
        "       X_grad = torch.cat(X_grads, dim=0)\n",
        "       weight_grad = sum(weight_grads) if weight_grads else None\n",
        "       ctx.save_for_backward(X_grad, weight_grad)\n",
        "       ctx.linear = linear\n",
        "       return output\n",
        "\n",
        "\n",
        "   @staticmethod\n",
        "   def backward(ctx, dY):\n",
        "       X_grad, weight_grad = ctx.saved_tensors\n",
        "       ctx.linear.weight.grad = weight_grad\n",
        "       return X_grad, None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "bSiaHjctUequ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Notes on Hugging Face Integration - Relevant for completing the llama_1B_training_loss_matches criterion later on\n",
        "\n",
        "Integrating this code with Hugging Face's CausalLM models can be quite fiddly. It took a **non-trivial amount of work** to figure out that these two key changes were necessary:\n",
        "\n",
        "1. **Normalization Factor for Loss**\n",
        "   - In the line:\n",
        "     ```python\n",
        "     chunk_loss /= num_chunks if not loss_from_HF else 1.0\n",
        "     ```\n",
        "   - The Llama model's loss function calls `fixed_cross_entropy` which already averages over the entire batch, so there is no need to correct the normalization factor manually.\n",
        "\n",
        "2. **Gradient Scaling in FP16**\n",
        "   - In the line:\n",
        "     ```python\n",
        "     if scaler is not None:\n",
        "         chunk_loss = scaler.scale(chunk_loss)\n",
        "     ```\n",
        "   - Hugging Face's `Trainer` scales the loss to prevent underflow in FP16 and vanishing gradients.\n",
        "   - This is not immediately obvious because the backward pass is initiated via `self.accelerator.backward(loss, **kwargs)`.\n",
        "   - To understand how `loss.backward()` is executed, I had to examine `accelerate/accelerator.py` and parse the right configuration."
      ],
      "metadata": {
        "id": "_B3ZarIsWDSz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lIczyn8-2-o"
      },
      "source": [
        "To test your implementation, it should not OOM for large inputs. Also, check the gradient is actually equivalent via `torch.allclose` in the normal approach."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import pandas as pd\n",
        "\n",
        "def track_memory_and_run(function, *args):\n",
        "   torch.cuda.reset_peak_memory_stats()\n",
        "   mem_before = torch.cuda.memory_allocated()\n",
        "   loss = function(*args)\n",
        "   loss.backward()\n",
        "   mem_after = torch.cuda.memory_allocated()\n",
        "   peak_memory = torch.cuda.max_memory_allocated()\n",
        "   return loss, mem_before, mem_after, peak_memory\n",
        "\n",
        "\n",
        "def store_and_reset_gradients(loss, batch, linear):\n",
        "   batch_grad, weight_grad = batch.grad.clone().cpu(), linear.weight.grad.clone().cpu()\n",
        "   batch.grad, linear.weight.grad = None, None\n",
        "   del loss\n",
        "   gc.collect()\n",
        "   torch.cuda.empty_cache()\n",
        "   return batch_grad, weight_grad\n",
        "\n",
        "def run_experiment(loss_fn, labels, split_size, batch, linear):\n",
        "   loss_naive, naive_mem_before, naive_mem_after, naive_peak_mem = track_memory_and_run(transformation_function, batch, linear, labels, loss_fn)\n",
        "   batch_grad_naive, weight_grad_naive = store_and_reset_gradients(loss_naive, batch, linear)\n",
        "   loss_efficient, efficient_mem_before, efficient_mem_after, efficient_peak_mem = track_memory_and_run(MemoryEfficientLinear.apply, batch, linear, labels, transformation_function, loss_fn, split_size)\n",
        "   batch_grad_efficient, weight_grad_efficient = store_and_reset_gradients(loss_efficient, batch, linear)\n",
        "\n",
        "   vram_reduction = (1 - (efficient_peak_mem - efficient_mem_before) / (naive_peak_mem - naive_mem_before)) * 100\n",
        "\n",
        "   print(f\"----- {loss_fn.__class__.__name__} results-----\")\n",
        "   memory_data = {\n",
        "       \"Memory (MB)\": [\"Before forward\", \"After backward\", \"Peak\"],\n",
        "       f\"Naive\": [naive_mem_before / 1024**2, naive_mem_after / 1024**2, naive_peak_mem / 1024**2],\n",
        "       f\"Efficient\": [efficient_mem_before / 1024**2, efficient_mem_after / 1024**2, efficient_peak_mem / 1024**2]\n",
        "   }\n",
        "   df = pd.DataFrame(memory_data)\n",
        "   print(df.to_string(index=False, col_space=15, justify='left'))\n",
        "   print(f\"\\nVRAM Reduction: {vram_reduction:.2f}%\")\n",
        "   print(f\"Are the losses close?: {torch.allclose(loss_naive, loss_efficient)}\")\n",
        "   print(f\"Are the input gradients close?: {torch.allclose(batch_grad_naive, batch_grad_efficient)}\")\n",
        "   print(f\"Are the weight gradients close?: {torch.allclose(weight_grad_naive, weight_grad_efficient)}\\n\")\n",
        "\n",
        "# Show 50% VRAM reduction\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "batch = torch.randn(8, 1024, 768, device=device, requires_grad=True)\n",
        "linear = nn.Linear(768, 2048, bias=False, device=device).to(device)\n",
        "split_size = 2\n",
        "run_experiment(nn.CrossEntropyLoss(reduction=\"mean\"), torch.randint(0, 2048, (8, 1024), device=device), split_size, batch, linear)\n",
        "\n",
        "#Show other loss functions work\n",
        "run_experiment(nn.MSELoss(reduction=\"mean\"), torch.randn(8, 1024, 2048, device=device), split_size, batch, linear)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAzsSC2aaVli",
        "outputId": "82199d01-526c-4117-948e-59ced37591c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- CrossEntropyLoss results-----\n",
            "Memory (MB)      Naive           Efficient     \n",
            "Before forward   30.062500       46.312988     \n",
            "After backward   76.312988       76.313477     \n",
            "          Peak  230.188477      130.315430     \n",
            "\n",
            "VRAM Reduction: 58.03%\n",
            "Are the losses close?: True\n",
            "Are the input gradients close?: True\n",
            "Are the weight gradients close?: True\n",
            "\n",
            "----- MSELoss results-----\n",
            "Memory (MB)      Naive           Efficient     \n",
            "Before forward  110.250000      174.250000     \n",
            "After backward  204.250000      204.250488     \n",
            "          Peak  302.250977      322.250488     \n",
            "\n",
            "VRAM Reduction: 22.92%\n",
            "Are the losses close?: True\n",
            "Are the input gradients close?: True\n",
            "Are the weight gradients close?: True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Llama 1B Training Loss Matches"
      ],
      "metadata": {
        "id": "GSAe6xPaaZhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training run for naive implementation"
      ],
      "metadata": {
        "id": "46oXClZQbmxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y9MVDnJ4x3d4",
        "outputId": "c9f23150-d8d0-473f-b067-fd3fe83f6a11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 50,\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        seed = 3407,\n",
        "        max_seq_length = max_seq_length,\n",
        "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
        "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
        "        report_to = \"none\", # For W&B\n",
        "        dataset_num_proc = 4,\n",
        "    ),\n",
        ")\n",
        "train_output = naive_trainer.train()"
      ],
      "metadata": {
        "id": "d2pctnZca3uE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97a162d4-03e0-4e16-caee-3212112e5380"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 01:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.039000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.101900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.186800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.221500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.967600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.575300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.213800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.767400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.150600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.853500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.986700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.997100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.866200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.998000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.655100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.776900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.109000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.872100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.667000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.911200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.940100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.166600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.894300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.709800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.700800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.454400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.808100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.151400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.472200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.897000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Qdg7erSWfSWA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training run for efficient implementation"
      ],
      "metadata": {
        "id": "9ZsIrE20bv7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF3L0SSgfj36",
        "outputId": "a4bb8f0a-1540-4326-d3ff-0abf40dcc8e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
        "from typing import Optional, Union, List\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "# Patch the forward pass for `LlamaForCausalLM` and pass the hidden states from `LlamaModel` to our efficient linear+loss.\n",
        "def efficient_forward(\n",
        "    self,\n",
        "    input_ids = None,\n",
        "    attention_mask = None,\n",
        "    position_ids = None,\n",
        "    past_key_values = None,\n",
        "    inputs_embeds = None,\n",
        "    labels = None,\n",
        "    use_cache = None,\n",
        "    output_attentions = None,\n",
        "    output_hidden_states = None,\n",
        "    return_dict = None,\n",
        "    cache_position = None,\n",
        "    logits_to_keep = 0,\n",
        "    **kwargs,\n",
        "):\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "        cache_position=cache_position,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    NO_SPLITS = 4\n",
        "\n",
        "    hidden_states = outputs[0]\n",
        "    split_size = hidden_states.shape[0] // NO_SPLITS\n",
        "    loss_fn = partial(model.loss_function, vocab_size=self.config.vocab_size, **kwargs)\n",
        "\n",
        "    logits = None\n",
        "    loss = MemoryEfficientLinear.apply(hidden_states,\n",
        "                                       self.lm_head,\n",
        "                                       labels,\n",
        "                                       transformation_function,\n",
        "                                       loss_fn,\n",
        "                                       split_size,\n",
        "                                       self.scaler)\n",
        "\n",
        "\n",
        "    if not return_dict:\n",
        "        return (loss, logits) + outputs[1:]\n",
        "\n",
        "    return CausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        past_key_values=outputs.past_key_values,\n",
        "        hidden_states=outputs.hidden_states,\n",
        "        attentions=outputs.attentions,\n",
        "    )\n",
        "\n",
        "\n",
        "# `accelerator` belongs to the `Trainer` object, but we need it to scale loss in our implementation\n",
        "# since we call compute gradients on the fly. Thus, we need to somehow reference it from within our efficient linear+loss.\n",
        "def patched_compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    model.model.scaler = self.accelerator.scaler\n",
        "    (loss, outputs) = super(SFTTrainer, self).compute_loss(\n",
        "        model, inputs, return_outputs=True, num_items_in_batch=num_items_in_batch\n",
        "    )\n",
        "\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "LlamaForCausalLM.forward = efficient_forward\n",
        "SFTTrainer.compute_loss = patched_compute_loss"
      ],
      "metadata": {
        "id": "tzGPevkurCwC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 50,\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        seed = 3407,\n",
        "        max_seq_length = max_seq_length,\n",
        "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
        "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
        "        report_to = \"none\", # For W&B\n",
        "        dataset_num_proc = 4,\n",
        "    ),\n",
        ")\n",
        "train_output = efficient_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "50baa3f9128e4b67b7925ec982700a3c",
            "17eb345535c14822a3f8eb3cde936a9e",
            "a8043030bf204dd29119f6ce419679ea",
            "b618c551348147ec96ef409e6c02c418",
            "bb645b61264048afabf307580e929408",
            "17fef94fa73e473baea47a60e6e2c4b2",
            "a0109e967ccb4c4e863cea9bd544dc16",
            "de44ff201dde48dcb932536b5a06fa8d",
            "9c7abaaf3a03438b85f461bb8704e089",
            "b61c9727dede478790e3fffb1bb0d70f",
            "1f43b4ab9c2145ae91e70bdb82afec9f",
            "5c53ff48c102430fb649aa0e4a1a66d4",
            "9cb57c6a71a349f48538c96b8e446a8f",
            "fd33cdfd84b84f619df22f2491c96fca",
            "279438c4562044fda1f0aafe731bcbf4",
            "8eebfcc3d4964121b794996c803e08c8",
            "1781303027f54df4a8d2fd971731264a",
            "e63398c5f713493db594ac99675bd818",
            "92647038507c45c08ac30985f3ebb620",
            "42d65c80c7834acda0a56ef9db544e3c",
            "ad18edaa45834f4baac6ef38d9fe8c4f",
            "03199a12b9774a3fa2d351581fa14b18",
            "5f5d28cd20404a61babc09999b8b4144",
            "5e2fca1d99e5440f808b4f137bd92b73",
            "d1f55922ad9c44fbb1797ac7e1af7670",
            "8ba3a360f4fe4d91b23720c63a9c4a5d",
            "fd245503192b4ea2b3c3f7b844ee5796",
            "7a03d97f9ebb497294f0fe3c5e0eb523",
            "31e50d9c9d3c4bd59f4c43dc403690ad",
            "7878cdee912e4bae9f96295f5d63a9cf",
            "a6cef594163e49d7b5a30c854d10df12",
            "d085ef373fc447208297b61b3d275bd8",
            "26fc9dd9adb94606b4e4e4a4963bf0a3"
          ]
        },
        "id": "lm7nCk5iunoD",
        "outputId": "3ee6bb57-3276-4e48-ac89-6628ce2d423f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50baa3f9128e4b67b7925ec982700a3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c53ff48c102430fb649aa0e4a1a66d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f5d28cd20404a61babc09999b8b4144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 01:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.356000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.038700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.102600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.221100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.967700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.574500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.214400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.766800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.859900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.150300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.043700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.788800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.854400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.853900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.987000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.996700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.866600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.825600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.044400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.997600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.655300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.958200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.776600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.109200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.872200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.911200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.940500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.166800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.709500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.454600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.887600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.490800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.151200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.471900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.897100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "naive_loss_values = [log[\"loss\"] for log in naive_trainer.state.log_history if \"loss\" in log]\n",
        "naive_steps = [log[\"step\"] for log in naive_trainer.state.log_history if \"loss\" in log]  # Ensure same length\n",
        "\n",
        "efficient_loss_values = [log[\"loss\"] for log in efficient_trainer.state.log_history if \"loss\" in log]\n",
        "efficient_steps = [log[\"step\"] for log in efficient_trainer.state.log_history if \"loss\" in log]  # Ensure same length\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(naive_steps, naive_loss_values, label=\"Training Loss (naive implementation)\", color=\"blue\", marker='+')\n",
        "plt.plot(efficient_steps, efficient_loss_values, label=\"Training Loss (efficient implementation) \", color=\"red\", alpha=0.5, marker='x')\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "s1l3Hp0Yg2Zq",
        "outputId": "b621bb22-c363-4553-ae7a-076ee9793653"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZ414R2Dk8M"
      },
      "source": [
        "## Marking Criteria for E) Max points = 10\n",
        "```python\n",
        "if attemped_E:\n",
        "    E_score = 0\n",
        "    if VRAM_50_percent_reduction: E_score += 2\n",
        "    if remove_float32_upcast: E_score = 0\n",
        "    if show_ce_loss_works: E_score += 1\n",
        "    if show_other_functions_work: E_score += 1\n",
        "    if hardcoded_gradients: E_score = 0\n",
        "    if allows_dynamic_chunk_sizes: E_score += 1\n",
        "    if llama_1B_training_loss_matches: E_score += 1\n",
        "    else: E_score = 0\n",
        "    if GRPO_memory_efficient_linear_works: E_score += 4\n",
        "    final_score += E_score\n",
        "else:\n",
        "    final_score += 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvrVlTmUN8nV"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"SUBMISSION\"></a>\n",
        "## Submission Steps\n",
        "\n",
        "1. All code should be in a public Github (Apache 2 Licensed)\n",
        "2. Kaggle notebooks and Colab notebooks should be linked in the README, and can be accessible through Colab / Kaggle.\n",
        "3. If attaching notebooks, must attach fully run ones - do not just add a notebook without running it. Kaggle notebook must be public, and run.\n",
        "4. Submit the Github to https://forms.gle/crSYnsGq3t1ck5TB9 If you want to send a private repo, please add me as a Github collaborate @danielhanchen\n",
        "5. Provide screenshots, graphs, plots, etc especially for training loss curves.\n",
        "6. We will comment and respond inside your Github repo. There will get 1 interview as well as a final step!\n",
        "\n",
        "### Clarifications:\n",
        "1. We'll compensate you if we interview you but don't hire you\n",
        "2. \\$100-\\$1000 bounties for Task 4\n",
        "3. Submissions must be Apache-2 licensed\n",
        "4. Task 4 involves solving Github issues for OSS Unsloth\n",
        "5. No time limit: rolling basis\n",
        "6. US based preferred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50baa3f9128e4b67b7925ec982700a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17eb345535c14822a3f8eb3cde936a9e",
              "IPY_MODEL_a8043030bf204dd29119f6ce419679ea",
              "IPY_MODEL_b618c551348147ec96ef409e6c02c418"
            ],
            "layout": "IPY_MODEL_bb645b61264048afabf307580e929408"
          }
        },
        "17eb345535c14822a3f8eb3cde936a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17fef94fa73e473baea47a60e6e2c4b2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a0109e967ccb4c4e863cea9bd544dc16",
            "value": "Applyingâ€‡chatâ€‡templateâ€‡toâ€‡trainâ€‡datasetâ€‡(num_proc=4):â€‡100%"
          }
        },
        "a8043030bf204dd29119f6ce419679ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de44ff201dde48dcb932536b5a06fa8d",
            "max": 21029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c7abaaf3a03438b85f461bb8704e089",
            "value": 21029
          }
        },
        "b618c551348147ec96ef409e6c02c418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b61c9727dede478790e3fffb1bb0d70f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f43b4ab9c2145ae91e70bdb82afec9f",
            "value": "â€‡21029/21029â€‡[00:06&lt;00:00,â€‡10070.31â€‡examples/s]"
          }
        },
        "bb645b61264048afabf307580e929408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17fef94fa73e473baea47a60e6e2c4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0109e967ccb4c4e863cea9bd544dc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de44ff201dde48dcb932536b5a06fa8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7abaaf3a03438b85f461bb8704e089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b61c9727dede478790e3fffb1bb0d70f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f43b4ab9c2145ae91e70bdb82afec9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c53ff48c102430fb649aa0e4a1a66d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cb57c6a71a349f48538c96b8e446a8f",
              "IPY_MODEL_fd33cdfd84b84f619df22f2491c96fca",
              "IPY_MODEL_279438c4562044fda1f0aafe731bcbf4"
            ],
            "layout": "IPY_MODEL_8eebfcc3d4964121b794996c803e08c8"
          }
        },
        "9cb57c6a71a349f48538c96b8e446a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1781303027f54df4a8d2fd971731264a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e63398c5f713493db594ac99675bd818",
            "value": "Tokenizingâ€‡trainâ€‡datasetâ€‡(num_proc=4):â€‡100%"
          }
        },
        "fd33cdfd84b84f619df22f2491c96fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92647038507c45c08ac30985f3ebb620",
            "max": 21029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42d65c80c7834acda0a56ef9db544e3c",
            "value": 21029
          }
        },
        "279438c4562044fda1f0aafe731bcbf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad18edaa45834f4baac6ef38d9fe8c4f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03199a12b9774a3fa2d351581fa14b18",
            "value": "â€‡21029/21029â€‡[00:13&lt;00:00,â€‡1693.85â€‡examples/s]"
          }
        },
        "8eebfcc3d4964121b794996c803e08c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1781303027f54df4a8d2fd971731264a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e63398c5f713493db594ac99675bd818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92647038507c45c08ac30985f3ebb620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d65c80c7834acda0a56ef9db544e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad18edaa45834f4baac6ef38d9fe8c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03199a12b9774a3fa2d351581fa14b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f5d28cd20404a61babc09999b8b4144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e2fca1d99e5440f808b4f137bd92b73",
              "IPY_MODEL_d1f55922ad9c44fbb1797ac7e1af7670",
              "IPY_MODEL_8ba3a360f4fe4d91b23720c63a9c4a5d"
            ],
            "layout": "IPY_MODEL_fd245503192b4ea2b3c3f7b844ee5796"
          }
        },
        "5e2fca1d99e5440f808b4f137bd92b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a03d97f9ebb497294f0fe3c5e0eb523",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_31e50d9c9d3c4bd59f4c43dc403690ad",
            "value": "Truncatingâ€‡trainâ€‡datasetâ€‡(num_proc=4):â€‡100%"
          }
        },
        "d1f55922ad9c44fbb1797ac7e1af7670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7878cdee912e4bae9f96295f5d63a9cf",
            "max": 21029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6cef594163e49d7b5a30c854d10df12",
            "value": 21029
          }
        },
        "8ba3a360f4fe4d91b23720c63a9c4a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d085ef373fc447208297b61b3d275bd8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_26fc9dd9adb94606b4e4e4a4963bf0a3",
            "value": "â€‡21029/21029â€‡[00:04&lt;00:00,â€‡4487.27â€‡examples/s]"
          }
        },
        "fd245503192b4ea2b3c3f7b844ee5796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a03d97f9ebb497294f0fe3c5e0eb523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e50d9c9d3c4bd59f4c43dc403690ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878cdee912e4bae9f96295f5d63a9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cef594163e49d7b5a30c854d10df12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d085ef373fc447208297b61b3d275bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26fc9dd9adb94606b4e4e4a4963bf0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}